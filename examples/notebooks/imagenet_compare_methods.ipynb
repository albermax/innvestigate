{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare analyzers on ImageNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we show how one can use **iNNvestigate** to analyze the prediction of ImageNet-models! To do so we will load a network from the keras.applications module and analyze prediction on some images!\n",
    "\n",
    "Parts of the code that do not contribute to the main focus are outsourced into utility modules. To learn more about the basic usage of **iNNvestigate** have look into this notebook: [Introduction to iNNvestigate](introduction.ipynb) and [Comparing methods on MNIST](mnist_method_comparison.ipynb)\n",
    "\n",
    "-----\n",
    "\n",
    "**To use this notebook please download the example images using the following script:**\n",
    "\n",
    "`innvestigate/examples/images/wget_imagenet_2011_samples.sh`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline  \n",
    "\n",
    "import imp\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import keras\n",
    "import keras.backend\n",
    "import keras.models\n",
    "\n",
    "import innvestigate\n",
    "import innvestigate.applications.imagenet\n",
    "import innvestigate.utils as iutils\n",
    "\n",
    "# Use utility libraries to focus on relevant iNNvestigate routines.\n",
    "eutils = imp.load_source(\"utils\", \"../utils.py\")\n",
    "imgnetutils = imp.load_source(\"utils_imagenet\", \"../utils_imagenet.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo use the VGG16-model, which uses ReLU activation layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the model definition.\n",
    "tmp = getattr(innvestigate.applications.imagenet, os.environ.get(\"NETWORKNAME\", \"vgg16\"))\n",
    "net = tmp(load_weights=True, load_patterns=\"relu\")\n",
    "\n",
    "# Build the model.\n",
    "model = keras.models.Model(inputs=net[\"in\"], outputs=net[\"sm_out\"])\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")\n",
    "\n",
    "# Handle input depending on model and backend.\n",
    "channels_first = keras.backend.image_data_format() == \"channels_first\"\n",
    "color_conversion = \"BGRtoRGB\" if net[\"color_coding\"] == \"BGR\" else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load some example images and preprocess them to fit the input size model.\n",
    "\n",
    "To analyze your own example images, just add them to `innvestigate/examples/images`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some example test set images.\n",
    "images, label_to_class_name = eutils.get_imagenet_data(net[\"image_shape\"][0])\n",
    "\n",
    "if not len(images):\n",
    "    raise Exception(\"Please download the example images using: \"\n",
    "                    \"'innvestigate/examples/images/wget_imagenet_2011_samples.sh'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will set up a list of analysis methods by preparing tuples containing the methods' string identifiers used by `innvestigate.analyzer.create_analyzer(...)`, some optional parameters, a post processing choice for visualizing the computed analysis and a title for the figure to render. Analyzers can be deactivated by simply commenting the corresponding lines, or added by creating a new tuple as below.\n",
    "\n",
    "For a full list of methods refer to the dictionary `investigate.analyzer.analyzers`.\n",
    "\n",
    "Note: Should you run into resource trouble, e.g. you are running that notebook on a computer without GPU or with only limited GPU memory, consider deactivating one or more analyzers by commenting the corresponding lines in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = net[\"patterns\"]\n",
    "input_range = net[\"input_range\"]\n",
    "\n",
    "# Methods we use and some properties.\n",
    "methods = [\n",
    "    # NAME                    OPT.PARAMS                POSTPROC FXN                TITLE\n",
    "    # Show input.\n",
    "    (\"input\",                 {},                       imgnetutils.image,         \"Input\"),\n",
    "\n",
    "    # Function\n",
    "    (\"gradient\",              {},                       imgnetutils.graymap,       \"Gradient\"),\n",
    "    (\"smoothgrad\",            {\"noise_scale\": 50},      imgnetutils.graymap,       \"SmoothGrad\"),\n",
    "    (\"integrated_gradients\",  {},                       imgnetutils.graymap,       \"Integrated Gradients\"),\n",
    "\n",
    "    # Signal\n",
    "    (\"deconvnet\",             {},                       imgnetutils.bk_proj,       \"Deconvnet\"),\n",
    "    (\"guided_backprop\",       {},                       imgnetutils.bk_proj,       \"Guided Backprop\",),\n",
    "    (\"pattern.net\",           {\"patterns\": patterns},   imgnetutils.bk_proj,       \"PatternNet\"),\n",
    "\n",
    "    # Interaction\n",
    "    (\"pattern.attribution\",   {\"patterns\": patterns},   imgnetutils.heatmap,       \"PatternAttribution\"),\n",
    "    (\"deep_taylor.bounded\",   {\"low\": input_range[0],\n",
    "                               \"high\": input_range[1]}, imgnetutils.heatmap,        \"DeepTaylor\"),\n",
    "    (\"lrp.z\",                 {},                       imgnetutils.heatmap,       \"LRP-Z\"),\n",
    "    (\"lrp.epsilon\",           {\"epsilon\": 1},           imgnetutils.heatmap,       \"LRP-Epsilon\"),\n",
    "    (\"lrp.sequential_preset_a_flat\",{\"epsilon\": 1},     imgnetutils.heatmap,       \"LRP-PresetAFlat\"),\n",
    "    (\"lrp.sequential_preset_b_flat\",{\"epsilon\": 1},     imgnetutils.heatmap,       \"LRP-PresetBFlat\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main loop below will now instantiate the analyzer objects based on the loaded/trained model and the analyzers' parameterizations above and compute the analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model without trailing softmax\n",
    "model_wo_softmax = iutils.keras.graph.model_wo_softmax(model)\n",
    "\n",
    "# Create analyzers.\n",
    "analyzers = []\n",
    "for method in methods:\n",
    "    try:\n",
    "        analyzer = innvestigate.create_analyzer(method[0],        # analysis method identifier\n",
    "                                                model_wo_softmax, # model without softmax output\n",
    "                                                **method[1])      # optional analysis parameters\n",
    "    except innvestigate.NotAnalyzeableModelException:\n",
    "        # Not all methods work with all models.\n",
    "        analyzer = None\n",
    "    analyzers.append(analyzer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we analyze each image with the different analyzers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = np.zeros([len(images), len(analyzers)]+net[\"image_shape\"]+[3])\n",
    "text = []\n",
    "\n",
    "for i, (x, y) in enumerate(images):\n",
    "    # Add batch axis.\n",
    "    x = x[None, :, :, :]\n",
    "    x_pp = imgnetutils.preprocess(x, net)\n",
    "\n",
    "    # Predict final activations, probabilites, and label.\n",
    "    presm = model_wo_softmax.predict_on_batch(x_pp)[0]\n",
    "    prob = model.predict_on_batch(x_pp)[0]\n",
    "    y_hat = prob.argmax()\n",
    "    \n",
    "    # Save prediction info:\n",
    "    text.append((\"%s\" % label_to_class_name[y],    # ground truth label\n",
    "                 \"%.2f\" % presm.max(),             # pre-softmax logits\n",
    "                 \"%.2f\" % prob.max(),              # probabilistic softmax output  \n",
    "                 \"%s\" % label_to_class_name[y_hat] # predicted label\n",
    "                ))\n",
    "\n",
    "    for aidx, analyzer in enumerate(analyzers):\n",
    "        if methods[aidx][0] == \"input\":\n",
    "            a = x\n",
    "        elif analyzer:\n",
    "            # Analyze.\n",
    "            a = analyzer.analyze(x_pp)\n",
    "\n",
    "            # Apply common postprocessing, e.g., re-ordering the channels for plotting.\n",
    "            a = imgnetutils.postprocess(a, color_conversion, channels_first)\n",
    "            # Apply analysis postprocessing, e.g., creating a heatmap.\n",
    "            a = methods[aidx][2](a)\n",
    "        else:\n",
    "            a = np.zeros_like(image)\n",
    "        # Store the analysis.\n",
    "        analysis[i, aidx] = a[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we visualize the analysis results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the grid as rectengular list\n",
    "grid = [[analysis[i, j] for j in range(analysis.shape[1])]\n",
    "        for i in range(analysis.shape[0])]  \n",
    "# Prepare the labels\n",
    "label, presm, prob, pred = zip(*text)\n",
    "row_labels_left = [('label: {}'.format(label[i]),'pred: {}'.format(pred[i])) for i in range(len(label))]\n",
    "row_labels_right = [('logit: {}'.format(presm[i]),'prob: {}'.format(prob[i])) for i in range(len(label))]\n",
    "col_labels = [''.join(method[3]) for method in methods]\n",
    "\n",
    "# Plot the analysis.\n",
    "eutils.plot_image_grid(grid, row_labels_left, row_labels_right, col_labels,\n",
    "                       file_name=os.environ.get(\"plot_file_name\", None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This figure shows the analysis regarding the *actually predicted* class as computed by the selected analyzers. Each column shows the visualized results for different analyzers and each row shows the analyses wrt to one input sample. To the left of each row, the ground truth label `label` and the predicted label `pred` are show. To the right, the model's probabilistic (softmax) output is shown as `prob` and the logit output just before the terminating softmax layer as `logit`. Note that all analyses have been performed based on the logit output (layer).\n",
    "\n",
    "\n",
    "If you are curious about how **iNNvestigate** performs on *different* ImageNet model, have a look here: [Comparing networks on ImageNet](imagenet_network_comparison.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
